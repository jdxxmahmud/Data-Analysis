{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n\n\n## I have learned the processes by following a course on Udemy\n## The link of the course is: https://www.udemy.com/course/data-analytics-real-world-case-studies-using-python/?src=sac&kw=data+analytics+real+world\n## Since I have learned everything from that course, I decided to add it as my notebook.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-14T12:11:28.277272Z","iopub.execute_input":"2022-02-14T12:11:28.277719Z","iopub.status.idle":"2022-02-14T12:11:28.291227Z","shell.execute_reply.started":"2022-02-14T12:11:28.277692Z","shell.execute_reply":"2022-02-14T12:11:28.290603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/jobs-on-naukricom/naukri_com-job_sample.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:28.292506Z","iopub.execute_input":"2022-02-14T12:11:28.292722Z","iopub.status.idle":"2022-02-14T12:11:28.783599Z","shell.execute_reply.started":"2022-02-14T12:11:28.29269Z","shell.execute_reply":"2022-02-14T12:11:28.78255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:28.785117Z","iopub.execute_input":"2022-02-14T12:11:28.785304Z","iopub.status.idle":"2022-02-14T12:11:28.801999Z","shell.execute_reply.started":"2022-02-14T12:11:28.785283Z","shell.execute_reply":"2022-02-14T12:11:28.800642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:28.8032Z","iopub.execute_input":"2022-02-14T12:11:28.803388Z","iopub.status.idle":"2022-02-14T12:11:28.818218Z","shell.execute_reply.started":"2022-02-14T12:11:28.803365Z","shell.execute_reply":"2022-02-14T12:11:28.81774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:28.819826Z","iopub.execute_input":"2022-02-14T12:11:28.820156Z","iopub.status.idle":"2022-02-14T12:11:28.833369Z","shell.execute_reply.started":"2022-02-14T12:11:28.820133Z","shell.execute_reply":"2022-02-14T12:11:28.832587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_missing = df.isnull().sum()\ncount_missing","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:28.834523Z","iopub.execute_input":"2022-02-14T12:11:28.834706Z","iopub.status.idle":"2022-02-14T12:11:28.862437Z","shell.execute_reply.started":"2022-02-14T12:11:28.834684Z","shell.execute_reply":"2022-02-14T12:11:28.86196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"percent_missing = (df.isnull().sum())* (100/(len(df)))\npercent_missing","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:28.863709Z","iopub.execute_input":"2022-02-14T12:11:28.86455Z","iopub.status.idle":"2022-02-14T12:11:28.906741Z","shell.execute_reply.started":"2022-02-14T12:11:28.864514Z","shell.execute_reply":"2022-02-14T12:11:28.905965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Making a dataframe from the missing values","metadata":{}},{"cell_type":"code","source":"missing_value_df = pd.DataFrame({\"count_missing\": count_missing,\n                                 \"percent_missing\": percent_missing})","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:28.907795Z","iopub.execute_input":"2022-02-14T12:11:28.907992Z","iopub.status.idle":"2022-02-14T12:11:28.913552Z","shell.execute_reply.started":"2022-02-14T12:11:28.907965Z","shell.execute_reply":"2022-02-14T12:11:28.912693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_value_df.style.background_gradient(cmap='Spectral')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:28.915147Z","iopub.execute_input":"2022-02-14T12:11:28.915615Z","iopub.status.idle":"2022-02-14T12:11:28.940487Z","shell.execute_reply.started":"2022-02-14T12:11:28.915576Z","shell.execute_reply":"2022-02-14T12:11:28.939815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Return unique values of all the columns","metadata":{}},{"cell_type":"code","source":"for col in df.columns:\n    print(f\"{col} has {df[col].nunique()} values\")","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:28.94151Z","iopub.execute_input":"2022-02-14T12:11:28.942473Z","iopub.status.idle":"2022-02-14T12:11:29.018017Z","shell.execute_reply.started":"2022-02-14T12:11:28.94244Z","shell.execute_reply":"2022-02-14T12:11:29.016764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### A list containing row name, number of unique values and the unique values","metadata":{}},{"cell_type":"code","source":"unique_df = [[col, df[col].nunique(), df[col].unique()] for col in df.columns]\nunique_df[:2]","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.020271Z","iopub.execute_input":"2022-02-14T12:11:29.020528Z","iopub.status.idle":"2022-02-14T12:11:29.08292Z","shell.execute_reply.started":"2022-02-14T12:11:29.020492Z","shell.execute_reply":"2022-02-14T12:11:29.08185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>We used list comprehension here. The code above works exactly like the following:\n    \n<code>\nunique = []\nfor col in df.columns:\n    unique.append([col, df[col].nunique(), df[col].unique()])\n</code>","metadata":{}},{"cell_type":"code","source":"count_df = pd.DataFrame(unique_df, columns=['col_name', 'count', 'unique'])\ncount_df.style.background_gradient(cmap='Spectral')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.084352Z","iopub.execute_input":"2022-02-14T12:11:29.084557Z","iopub.status.idle":"2022-02-14T12:11:29.099339Z","shell.execute_reply.started":"2022-02-14T12:11:29.084535Z","shell.execute_reply":"2022-02-14T12:11:29.09894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Cleaning the 'payrate' feature","metadata":{}},{"cell_type":"code","source":"df['payrate']","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.100249Z","iopub.execute_input":"2022-02-14T12:11:29.100512Z","iopub.status.idle":"2022-02-14T12:11:29.124574Z","shell.execute_reply.started":"2022-02-14T12:11:29.100479Z","shell.execute_reply":"2022-02-14T12:11:29.124079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here I will find the following for each row<br>\n1. Minimum Payrate<br>\n2. Maximum Payrate<br>","metadata":{}},{"cell_type":"code","source":"# The minimum and maximum payrate\ndf['payrate'][0].split(' - ')[0]","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.125427Z","iopub.execute_input":"2022-02-14T12:11:29.125672Z","iopub.status.idle":"2022-02-14T12:11:29.137262Z","shell.execute_reply.started":"2022-02-14T12:11:29.125643Z","shell.execute_reply":"2022-02-14T12:11:29.136624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_and_max = df['payrate'][0].split(' - ')\n# min_and_max =>  Gives output ['1,50,000', '2,25,000 P.A']\n\n# We will remove ' P.A' from the second i.e. the maximum payrate, so that we get two numbers from min_and_max variable \nmin_and_max[1] = min_and_max[1].split(' ')[0]   # This gives '2,25,000'\n\nmin_and_max","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.138258Z","iopub.execute_input":"2022-02-14T12:11:29.13858Z","iopub.status.idle":"2022-02-14T12:11:29.153857Z","shell.execute_reply.started":"2022-02-14T12:11:29.138554Z","shell.execute_reply":"2022-02-14T12:11:29.153028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What we have done above only applies for those cells that have a minimum and maximum range. Other than that, the rest will have to be processed diffrently.<br>\nHere we can say that, after <i>splitting</i>, if there are two values in the list, the values will surely be minimum and maximum payrate (exactly like <i><u>min_and_max</u></i>). So the length of those valid cells will be 2.<br>\nWe can easily find how many valid cells are there using the following method:","metadata":{}},{"cell_type":"code","source":"'''\nWe are creating a list that will have the lengths of the lists after splitting the string around '-'\n'''\nlen_pay = []\nfor pay in df['payrate']:\n    len_pay.append(len(str(pay).split('-')))","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.15541Z","iopub.execute_input":"2022-02-14T12:11:29.155674Z","iopub.status.idle":"2022-02-14T12:11:29.177257Z","shell.execute_reply.started":"2022-02-14T12:11:29.155639Z","shell.execute_reply":"2022-02-14T12:11:29.176233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.Series(len_pay).value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.17883Z","iopub.execute_input":"2022-02-14T12:11:29.179135Z","iopub.status.idle":"2022-02-14T12:11:29.198018Z","shell.execute_reply.started":"2022-02-14T12:11:29.179103Z","shell.execute_reply":"2022-02-14T12:11:29.197253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we have 4682 values that are valid entries for such processing","metadata":{}},{"cell_type":"markdown","source":"Here we will create a feature table that will hold all the ranges.<br>\nFor example, if we have a payrate like 1k-2k-3k-4k, we have to store 1k, 2k, 3k, 4k in different columns.<br>\nThis will be an optimal solution. ","metadata":{}},{"cell_type":"code","source":"payrate_split = df['payrate'].str.split('-', expand=True)   \n\n'''\nBy default the value of expand is False\n\nThe expand parameter will expand the values in different columns as following\n'''\n\npayrate_split","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.199078Z","iopub.execute_input":"2022-02-14T12:11:29.199739Z","iopub.status.idle":"2022-02-14T12:11:29.250081Z","shell.execute_reply.started":"2022-02-14T12:11:29.19966Z","shell.execute_reply":"2022-02-14T12:11:29.249307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To properly convert the entries into integers, we have to: <br>\n1. Remove extra spaces<br>\n2. Remove commas","metadata":{}},{"cell_type":"code","source":"payrate_split[0][0]","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.251005Z","iopub.execute_input":"2022-02-14T12:11:29.251165Z","iopub.status.idle":"2022-02-14T12:11:29.256728Z","shell.execute_reply.started":"2022-02-14T12:11:29.251135Z","shell.execute_reply":"2022-02-14T12:11:29.256271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\npayrate_split[0] = payrate_split[0].str.strip()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.257912Z","iopub.execute_input":"2022-02-14T12:11:29.258059Z","iopub.status.idle":"2022-02-14T12:11:29.276178Z","shell.execute_reply.started":"2022-02-14T12:11:29.258038Z","shell.execute_reply":"2022-02-14T12:11:29.275401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"payrate_split[0].str.replace(',', '')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.277084Z","iopub.execute_input":"2022-02-14T12:11:29.277246Z","iopub.status.idle":"2022-02-14T12:11:29.301494Z","shell.execute_reply.started":"2022-02-14T12:11:29.277223Z","shell.execute_reply":"2022-02-14T12:11:29.301119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can use <b>Lambda</b> function to replace comma will an empty string for each entry<br>\nIt will do the same as the previous cell did.","metadata":{}},{"cell_type":"code","source":"payrate_split[0] = payrate_split[0].apply(lambda x: str(x).replace(',', ''))\n\npayrate_split[0]","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.30239Z","iopub.execute_input":"2022-02-14T12:11:29.302624Z","iopub.status.idle":"2022-02-14T12:11:29.323598Z","shell.execute_reply.started":"2022-02-14T12:11:29.302599Z","shell.execute_reply":"2022-02-14T12:11:29.323123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have to find whether there are any float values or not.<br>\nAlso we have to deal with the string values. ","metadata":{}},{"cell_type":"markdown","source":"Now I will get the <b>minimum pay</b> if there is float or int values<br>\nWe have to extract or seperate the values from the features.<br>\nPossible ways:<br>\n1. Exception Handling/ try except block\n2. Regular expression\n3. <i>to_numeric()</i> function in Pandas\n4. Any function along with <i>map</i> function.","metadata":{"execution":{"iopub.status.busy":"2022-01-24T18:36:01.581082Z","iopub.execute_input":"2022-01-24T18:36:01.581309Z","iopub.status.idle":"2022-01-24T18:36:01.585685Z","shell.execute_reply.started":"2022-01-24T18:36:01.58128Z","shell.execute_reply":"2022-01-24T18:36:01.584556Z"}}},{"cell_type":"markdown","source":"#### 1. Exception\n\n> In the <i>try block</i>, there will be an <b>if condition</b> that will check whether the value is float. If not, then that is the missing value\n\n<br>\n\n> try:<br>\n> &emsp; if dtype(payrate) == float<br>\n> except:<br>\n> &emsp; \"Missing Value\"","metadata":{}},{"cell_type":"code","source":"pay = []      # This will contain the converted values of the first column of \"payrate_split\"\nfor payrate in payrate_split[0]:\n    try:\n        if type(float(payrate)) == np.float:\n            pay.append(payrate)\n    except:\n            pay.append(\"missing value\")\n","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.324565Z","iopub.execute_input":"2022-02-14T12:11:29.32485Z","iopub.status.idle":"2022-02-14T12:11:29.36752Z","shell.execute_reply.started":"2022-02-14T12:11:29.324814Z","shell.execute_reply":"2022-02-14T12:11:29.367063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2. to_numeric function","metadata":{}},{"cell_type":"code","source":"pd.to_numeric(payrate_split[0], errors='coerce')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.368471Z","iopub.execute_input":"2022-02-14T12:11:29.368655Z","iopub.status.idle":"2022-02-14T12:11:29.392032Z","shell.execute_reply.started":"2022-02-14T12:11:29.368627Z","shell.execute_reply":"2022-02-14T12:11:29.391283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3. Regular Expression\n\n> pattern = '\\D.*' <br>\n> This means anything other than digits\n\nSo if I get anything other than digits, I will replace it will an empty string","metadata":{}},{"cell_type":"code","source":"# A pattern that contains no numeric values\npattern = '\\D.*'","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.393579Z","iopub.execute_input":"2022-02-14T12:11:29.393859Z","iopub.status.idle":"2022-02-14T12:11:29.403337Z","shell.execute_reply.started":"2022-02-14T12:11:29.393825Z","shell.execute_reply":"2022-02-14T12:11:29.402811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"payrate_split[0].str.replace(pattern, '')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.404233Z","iopub.execute_input":"2022-02-14T12:11:29.405397Z","iopub.status.idle":"2022-02-14T12:11:29.439887Z","shell.execute_reply.started":"2022-02-14T12:11:29.405339Z","shell.execute_reply":"2022-02-14T12:11:29.43921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4. \"any\" and \"map\"","metadata":{}},{"cell_type":"code","source":"any(map(str.isdigit, payrate_split[0]))","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.440802Z","iopub.execute_input":"2022-02-14T12:11:29.440987Z","iopub.status.idle":"2022-02-14T12:11:29.448815Z","shell.execute_reply.started":"2022-02-14T12:11:29.440963Z","shell.execute_reply":"2022-02-14T12:11:29.447907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"any(map(str.isnumeric, payrate_split[0]))","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.449878Z","iopub.execute_input":"2022-02-14T12:11:29.450437Z","iopub.status.idle":"2022-02-14T12:11:29.461632Z","shell.execute_reply.started":"2022-02-14T12:11:29.450404Z","shell.execute_reply":"2022-02-14T12:11:29.461165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will look and do something with the remaining columns of <i>payrate_split</i> <br>\nThe second column, or <b>payrate_split[1]</b> will have the max value. Here we have to follow some steps, which are: <br>\n1. Removing all the extra white spaces.\n2. Removing all the commas.\n3. Removing set of characters.\n4. Changing data type to float/int.\n","metadata":{}},{"cell_type":"code","source":"payrate_split[1] = payrate_split[1].str.strip()\npayrate_split[1]","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.465699Z","iopub.execute_input":"2022-02-14T12:11:29.465977Z","iopub.status.idle":"2022-02-14T12:11:29.481383Z","shell.execute_reply.started":"2022-02-14T12:11:29.465955Z","shell.execute_reply":"2022-02-14T12:11:29.480187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"payrate_split[1] = payrate_split[1].apply(lambda x: str(x).replace(',', ''))","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.482597Z","iopub.execute_input":"2022-02-14T12:11:29.483118Z","iopub.status.idle":"2022-02-14T12:11:29.503156Z","shell.execute_reply.started":"2022-02-14T12:11:29.483083Z","shell.execute_reply":"2022-02-14T12:11:29.502111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pattern = '\\D.*'\n\npayrate_split[1] = payrate_split[1].str.replace(pattern, '')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.504266Z","iopub.execute_input":"2022-02-14T12:11:29.504466Z","iopub.status.idle":"2022-02-14T12:11:29.536807Z","shell.execute_reply.started":"2022-02-14T12:11:29.504443Z","shell.execute_reply":"2022-02-14T12:11:29.536375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"payrate_split.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.537661Z","iopub.execute_input":"2022-02-14T12:11:29.537858Z","iopub.status.idle":"2022-02-14T12:11:29.543993Z","shell.execute_reply.started":"2022-02-14T12:11:29.537829Z","shell.execute_reply":"2022-02-14T12:11:29.543136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"payrate_split[0] = pd.to_numeric(payrate_split[0], errors='coerce')  # For the minimum pay feature\npayrate_split[1] = pd.to_numeric(payrate_split[1], errors='coerce')  # For the maximum pay feature","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.544927Z","iopub.execute_input":"2022-02-14T12:11:29.545105Z","iopub.status.idle":"2022-02-14T12:11:29.58028Z","shell.execute_reply.started":"2022-02-14T12:11:29.545082Z","shell.execute_reply":"2022-02-14T12:11:29.57974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"payrate_split.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.581118Z","iopub.execute_input":"2022-02-14T12:11:29.581358Z","iopub.status.idle":"2022-02-14T12:11:29.586399Z","shell.execute_reply.started":"2022-02-14T12:11:29.581336Z","shell.execute_reply":"2022-02-14T12:11:29.58554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Now we have to insert this minimum and maximum payrate in the main <i>df</i> dataframe which can be done in two ways. \n\n1. Defining new column in *df* for minimum and maximum payrate. (When you are adding below 5 features, just like now)\n2. Concatenation Approach, using pandas library. (When you are adding more than 5-10 features)\n\n","metadata":{}},{"cell_type":"markdown","source":"#### Defining new columns","metadata":{}},{"cell_type":"code","source":"# df['min_pay'] = payrate_split[0]\n# df['max_pay'] = payrate_split[1]","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.587781Z","iopub.execute_input":"2022-02-14T12:11:29.588589Z","iopub.status.idle":"2022-02-14T12:11:29.60017Z","shell.execute_reply.started":"2022-02-14T12:11:29.588547Z","shell.execute_reply":"2022-02-14T12:11:29.599335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Concatenation using *pd.concat()* function","metadata":{}},{"cell_type":"code","source":"pay = pd.concat([payrate_split[0], payrate_split[1]], axis=1, sort=False)\npay.columns = ['min_pay', 'max_pay']\npay","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.601123Z","iopub.execute_input":"2022-02-14T12:11:29.601884Z","iopub.status.idle":"2022-02-14T12:11:29.622874Z","shell.execute_reply.started":"2022-02-14T12:11:29.601843Z","shell.execute_reply":"2022-02-14T12:11:29.621802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([df, pay], axis=1, sort=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.623989Z","iopub.execute_input":"2022-02-14T12:11:29.624504Z","iopub.status.idle":"2022-02-14T12:11:29.633686Z","shell.execute_reply.started":"2022-02-14T12:11:29.624468Z","shell.execute_reply":"2022-02-14T12:11:29.633182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.634457Z","iopub.execute_input":"2022-02-14T12:11:29.634907Z","iopub.status.idle":"2022-02-14T12:11:29.658135Z","shell.execute_reply.started":"2022-02-14T12:11:29.63488Z","shell.execute_reply":"2022-02-14T12:11:29.657588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cleaning and featurizing *'experience'*  feature","metadata":{}},{"cell_type":"code","source":"df['experience'][0]","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.659142Z","iopub.execute_input":"2022-02-14T12:11:29.659439Z","iopub.status.idle":"2022-02-14T12:11:29.664575Z","shell.execute_reply.started":"2022-02-14T12:11:29.659414Z","shell.execute_reply":"2022-02-14T12:11:29.663923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# exp_list = list(df['experience'].apply(lambda x: str(x)[:-3:].strip().split(' - ')))\n\n# exp_len = [len(i) for i in exp_list]\n# # exp_list, ex_len\n","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.665503Z","iopub.execute_input":"2022-02-14T12:11:29.666042Z","iopub.status.idle":"2022-02-14T12:11:29.679298Z","shell.execute_reply.started":"2022-02-14T12:11:29.666016Z","shell.execute_reply":"2022-02-14T12:11:29.678546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"    def split_exp(exp):    \n        min_exp = exp.split('-')[0]\n        max_exp = exp.split('-')[1]\n    \n        return min_exp, max_exp\n        \n        \n  We could have done the above. But we won't always have sweet data as '1 - 3 year', we may have **null** values or **1 - 3 - 5 years** type values. <br>\n  So what we have to do here is as following:\n","metadata":{"execution":{"iopub.status.busy":"2022-02-11T17:45:10.271474Z","iopub.execute_input":"2022-02-11T17:45:10.271984Z","iopub.status.idle":"2022-02-11T17:45:10.276742Z","shell.execute_reply.started":"2022-02-11T17:45:10.271949Z","shell.execute_reply":"2022-02-11T17:45:10.276055Z"}}},{"cell_type":"code","source":"len1 = []\n\nfor exp in df['experience'].dropna():\n    if len(exp.split('-')) != 2:\n        len1.append(exp)\n        \nlen1","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.680255Z","iopub.execute_input":"2022-02-14T12:11:29.680687Z","iopub.status.idle":"2022-02-14T12:11:29.703247Z","shell.execute_reply.started":"2022-02-14T12:11:29.680644Z","shell.execute_reply":"2022-02-14T12:11:29.702807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### I can drop all the rows having \"Not Mentioned\", which is not a handy approach. Other than this, there is also an optimal solution which is enhancing split_exp function by using \"exception handling\"","metadata":{}},{"cell_type":"code","source":"def split_exp(exp):\n    try:\n        if len(exp.split('-')) == 2:\n            min_exp = exp.split('-')[0]\n            max_exp = exp.split('-')[1]\n        return pd.Series([min_exp, max_exp])\n    except:\n        return pd.Series([np.nan, np.nan])\n    \n# To append the new data easily, we are returning pd.Series() data here. ","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.704248Z","iopub.execute_input":"2022-02-14T12:11:29.704421Z","iopub.status.idle":"2022-02-14T12:11:29.709568Z","shell.execute_reply.started":"2022-02-14T12:11:29.704392Z","shell.execute_reply":"2022-02-14T12:11:29.708735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['min_exp', 'max_exp']] = df['experience'].apply(split_exp).rename(columns={0:'min_exp', 1:'max_exp'})","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:29.710822Z","iopub.execute_input":"2022-02-14T12:11:29.711212Z","iopub.status.idle":"2022-02-14T12:11:35.148985Z","shell.execute_reply.started":"2022-02-14T12:11:29.711186Z","shell.execute_reply":"2022-02-14T12:11:35.148097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will talk about the rows where the experience value is **Not Mentioned**<br>\nI will create another DataFrame for that and filter out those peculiar values.<br>\nWe will store those values in *nm* variable which means *not mentioned*","metadata":{}},{"cell_type":"code","source":"nm = pd.DataFrame(df['experience'].str.contains(\"Not Mentioned\"))\n\nnm[nm['experience'] == True]","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:35.150131Z","iopub.execute_input":"2022-02-14T12:11:35.150389Z","iopub.status.idle":"2022-02-14T12:11:35.1741Z","shell.execute_reply.started":"2022-02-14T12:11:35.150352Z","shell.execute_reply":"2022-02-14T12:11:35.172933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now if I want to get the indices of these values, then I will add *.index* after the previous line of code.","metadata":{}},{"cell_type":"code","source":"nm[nm['experience'] == True].index","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:35.175322Z","iopub.execute_input":"2022-02-14T12:11:35.175505Z","iopub.status.idle":"2022-02-14T12:11:35.192639Z","shell.execute_reply.started":"2022-02-14T12:11:35.175482Z","shell.execute_reply":"2022-02-14T12:11:35.191897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above values are indices of those rows with values \"Not Mentioned\".<br>\nNow if I want to put this value into the funtion **split_exp**, I will get NaN values. ","metadata":{}},{"cell_type":"code","source":"nm['experience'][1138]","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:35.193585Z","iopub.execute_input":"2022-02-14T12:11:35.193788Z","iopub.status.idle":"2022-02-14T12:11:35.202055Z","shell.execute_reply.started":"2022-02-14T12:11:35.193735Z","shell.execute_reply":"2022-02-14T12:11:35.201656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The result of this code will be NaN. Because the returned values causes an exception.\nsplit_exp(df['experience'][1138])","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:35.202742Z","iopub.execute_input":"2022-02-14T12:11:35.203522Z","iopub.status.idle":"2022-02-14T12:11:35.215611Z","shell.execute_reply.started":"2022-02-14T12:11:35.203497Z","shell.execute_reply":"2022-02-14T12:11:35.215232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:35.217504Z","iopub.execute_input":"2022-02-14T12:11:35.218018Z","iopub.status.idle":"2022-02-14T12:11:35.244771Z","shell.execute_reply.started":"2022-02-14T12:11:35.217978Z","shell.execute_reply":"2022-02-14T12:11:35.243801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now removing *'yrs'* from the **max_exp** column and update the column","metadata":{}},{"cell_type":"code","source":"df['max_exp'] = df['max_exp'].str.replace('yrs', '')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:35.246252Z","iopub.execute_input":"2022-02-14T12:11:35.24671Z","iopub.status.idle":"2022-02-14T12:11:35.276615Z","shell.execute_reply.started":"2022-02-14T12:11:35.246677Z","shell.execute_reply":"2022-02-14T12:11:35.276144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:35.277637Z","iopub.execute_input":"2022-02-14T12:11:35.278514Z","iopub.status.idle":"2022-02-14T12:11:35.29944Z","shell.execute_reply.started":"2022-02-14T12:11:35.278447Z","shell.execute_reply":"2022-02-14T12:11:35.298743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:35.300715Z","iopub.execute_input":"2022-02-14T12:11:35.300943Z","iopub.status.idle":"2022-02-14T12:11:35.309371Z","shell.execute_reply.started":"2022-02-14T12:11:35.300915Z","shell.execute_reply":"2022-02-14T12:11:35.308405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will convert min_exp and max_exp to float","metadata":{}},{"cell_type":"code","source":"df['max_exp'] = df['max_exp'].astype(float)\ndf['min_exp'] = df['min_exp'].astype(float)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:35.31102Z","iopub.execute_input":"2022-02-14T12:11:35.311237Z","iopub.status.idle":"2022-02-14T12:11:35.329673Z","shell.execute_reply.started":"2022-02-14T12:11:35.311208Z","shell.execute_reply":"2022-02-14T12:11:35.328913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['avg_experience'] = (df['min_exp'] + df['max_exp'])/2\ndf['avg_payrate'] = (df['min_pay'] + df['max_pay'])/2","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:35.331195Z","iopub.execute_input":"2022-02-14T12:11:35.331553Z","iopub.status.idle":"2022-02-14T12:11:35.339633Z","shell.execute_reply.started":"2022-02-14T12:11:35.331527Z","shell.execute_reply":"2022-02-14T12:11:35.338662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the **min_pay, max_pay** columns and **min_exp, max_exp** columns, avg_pay and avg_exp can be derived. Which will give us more insight on the data.","metadata":{}},{"cell_type":"markdown","source":"### Perform Feature Engineering on **postdate** feature","metadata":{}},{"cell_type":"markdown","source":"##### Approaches:\n1. Define own function.\n2. Inbuilt functions, using datetime module\n3. Optimal way: Lambda function\n4. map function","metadata":{}},{"cell_type":"markdown","source":"#### Approach 1. Define own function","metadata":{}},{"cell_type":"code","source":"df['postdate']","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:35.340878Z","iopub.execute_input":"2022-02-14T12:11:35.341084Z","iopub.status.idle":"2022-02-14T12:11:35.353905Z","shell.execute_reply.started":"2022-02-14T12:11:35.341059Z","shell.execute_reply":"2022-02-14T12:11:35.352992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 2 types of datetime datatypes:\n1. datetime64[ns]\n2. <M8[ns]\n\nboth are basically same. But the entire structure depends on how numpy datatype is designed. <br>\nIn the following code we can see that both the datatypes are same.","metadata":{}},{"cell_type":"code","source":"np.dtype('datetime64[ns]') == np.dtype('<M8[ns]')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:35.355128Z","iopub.execute_input":"2022-02-14T12:11:35.35549Z","iopub.status.idle":"2022-02-14T12:11:35.364731Z","shell.execute_reply.started":"2022-02-14T12:11:35.355458Z","shell.execute_reply":"2022-02-14T12:11:35.36434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['postdate'].dtype","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:35.366015Z","iopub.execute_input":"2022-02-14T12:11:35.366392Z","iopub.status.idle":"2022-02-14T12:11:35.378859Z","shell.execute_reply.started":"2022-02-14T12:11:35.366366Z","shell.execute_reply":"2022-02-14T12:11:35.37801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here I will write a function that will get day, month and year from each row.","metadata":{}},{"cell_type":"code","source":"def fetch_dt_att(dataframe, feature):\n    try:\n       return pd.Series([dataframe[feature].dt.day, dataframe[feature].dt.month, dataframe[feature].dt.year])\n    except:\n        print('Data type is not supported')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:35.380265Z","iopub.execute_input":"2022-02-14T12:11:35.380609Z","iopub.status.idle":"2022-02-14T12:11:35.388792Z","shell.execute_reply.started":"2022-02-14T12:11:35.380576Z","shell.execute_reply":"2022-02-14T12:11:35.388114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fetch_dt_att(df, 'postdate')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:35.389989Z","iopub.execute_input":"2022-02-14T12:11:35.390308Z","iopub.status.idle":"2022-02-14T12:11:35.40659Z","shell.execute_reply.started":"2022-02-14T12:11:35.390276Z","shell.execute_reply":"2022-02-14T12:11:35.405359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have not changed the data-type of the **postdate** feature. We will do it here. ","metadata":{}},{"cell_type":"code","source":"df['postdate'] = pd.to_datetime(df['postdate'])","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:35.408774Z","iopub.execute_input":"2022-02-14T12:11:35.409364Z","iopub.status.idle":"2022-02-14T12:11:35.430296Z","shell.execute_reply.started":"2022-02-14T12:11:35.409336Z","shell.execute_reply":"2022-02-14T12:11:35.429214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['day', 'month', 'year']] = fetch_dt_att(df, 'postdate')\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:35.431568Z","iopub.execute_input":"2022-02-14T12:11:35.43189Z","iopub.status.idle":"2022-02-14T12:11:35.462543Z","shell.execute_reply.started":"2022-02-14T12:11:35.431865Z","shell.execute_reply":"2022-02-14T12:11:35.46181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Approach 3. Using 'Lambda'","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Approach 4. Use 'map' function","metadata":{}},{"cell_type":"markdown","source":"First I will define the function","metadata":{}},{"cell_type":"code","source":"def fetch_dt_att2(x):\n    return ([x.day, x.month, x.year])","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:35.463686Z","iopub.execute_input":"2022-02-14T12:11:35.464241Z","iopub.status.idle":"2022-02-14T12:11:35.469517Z","shell.execute_reply.started":"2022-02-14T12:11:35.464203Z","shell.execute_reply":"2022-02-14T12:11:35.467626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fe_date = pd.DataFrame(map(fetch_dt_att2, df['postdate'])).rename(columns={0: 'day', 1: 'month', 2: 'year'})\nfe_date","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:35.470731Z","iopub.execute_input":"2022-02-14T12:11:35.471145Z","iopub.status.idle":"2022-02-14T12:11:35.538595Z","shell.execute_reply.started":"2022-02-14T12:11:35.471117Z","shell.execute_reply":"2022-02-14T12:11:35.537899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can concatenate this **fe_date** with df and will get the 3 columns. Since we have already done that using the previous steps, here I will not do it.<br>\nFollowing is the code for it.","metadata":{}},{"cell_type":"code","source":"# pd.concat([df, fe_date], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:35.539544Z","iopub.execute_input":"2022-02-14T12:11:35.540146Z","iopub.status.idle":"2022-02-14T12:11:35.543956Z","shell.execute_reply.started":"2022-02-14T12:11:35.540117Z","shell.execute_reply":"2022-02-14T12:11:35.543432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prepare job_location feature","metadata":{}},{"cell_type":"code","source":"df['joblocation_address'].value_counts().head(10)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:35.545149Z","iopub.execute_input":"2022-02-14T12:11:35.545315Z","iopub.status.idle":"2022-02-14T12:11:35.563708Z","shell.execute_reply.started":"2022-02-14T12:11:35.545292Z","shell.execute_reply":"2022-02-14T12:11:35.563108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Make a copy so that whatever manipulation is done, can be reverted back.","metadata":{}},{"cell_type":"code","source":"data = df.copy()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:35.565118Z","iopub.execute_input":"2022-02-14T12:11:35.566543Z","iopub.status.idle":"2022-02-14T12:11:35.582191Z","shell.execute_reply.started":"2022-02-14T12:11:35.566476Z","shell.execute_reply":"2022-02-14T12:11:35.581421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.Series(data['joblocation_address'].unique())","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:35.583456Z","iopub.execute_input":"2022-02-14T12:11:35.583787Z","iopub.status.idle":"2022-02-14T12:11:35.59207Z","shell.execute_reply.started":"2022-02-14T12:11:35.583732Z","shell.execute_reply":"2022-02-14T12:11:35.591607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['joblocation_address'].value_counts().head(60).index","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:35.592817Z","iopub.execute_input":"2022-02-14T12:11:35.593314Z","iopub.status.idle":"2022-02-14T12:11:35.608572Z","shell.execute_reply.started":"2022-02-14T12:11:35.593283Z","shell.execute_reply":"2022-02-14T12:11:35.607728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that, there are some data repeatations. <br>\nSome replacements are needed, which will be stored in dictionary. <br>\nI will upload another csv file that will have the replacements.<br>\nAfter this processing, the **duplicate subcategories** will mostly be removed. ","metadata":{}},{"cell_type":"code","source":"rep = pd.read_csv(r'/kaggle/input/d/junaidmahmud/replacements/replacements.csv').set_index('Unnamed: 0')\nrep","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:35.610569Z","iopub.execute_input":"2022-02-14T12:11:35.61085Z","iopub.status.idle":"2022-02-14T12:11:35.63018Z","shell.execute_reply.started":"2022-02-14T12:11:35.610819Z","shell.execute_reply":"2022-02-14T12:11:35.629117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"replacement_dict = rep.to_dict()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:35.63143Z","iopub.execute_input":"2022-02-14T12:11:35.632198Z","iopub.status.idle":"2022-02-14T12:11:35.636844Z","shell.execute_reply.started":"2022-02-14T12:11:35.632158Z","shell.execute_reply":"2022-02-14T12:11:35.63543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.replace(replacement_dict, inplace=True, regex=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:35.638208Z","iopub.execute_input":"2022-02-14T12:11:35.638437Z","iopub.status.idle":"2022-02-14T12:11:36.606547Z","shell.execute_reply.started":"2022-02-14T12:11:35.638407Z","shell.execute_reply":"2022-02-14T12:11:36.605886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['joblocation_address'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:36.607882Z","iopub.execute_input":"2022-02-14T12:11:36.608105Z","iopub.status.idle":"2022-02-14T12:11:36.619735Z","shell.execute_reply.started":"2022-02-14T12:11:36.608075Z","shell.execute_reply":"2022-02-14T12:11:36.618813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['joblocation_address'].value_counts().head(10)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:36.621098Z","iopub.execute_input":"2022-02-14T12:11:36.621878Z","iopub.status.idle":"2022-02-14T12:11:36.633692Z","shell.execute_reply.started":"2022-02-14T12:11:36.621845Z","shell.execute_reply":"2022-02-14T12:11:36.632926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Now I will get some data using city names\n\nThat can be done using the following ways:\n1. filter\n2. Query\n3. isin function","metadata":{}},{"cell_type":"markdown","source":"#### Approach 1. Filter","metadata":{}},{"cell_type":"code","source":"loc_filter = df['joblocation_address'] == 'Noida'\nloc_noida = df[loc_filter]\n\nloc_noida.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:36.634714Z","iopub.execute_input":"2022-02-14T12:11:36.634969Z","iopub.status.idle":"2022-02-14T12:11:36.676385Z","shell.execute_reply.started":"2022-02-14T12:11:36.634945Z","shell.execute_reply":"2022-02-14T12:11:36.675467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['joblocation_address'][1760], data['joblocation_address'][1760]","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:36.67754Z","iopub.execute_input":"2022-02-14T12:11:36.677716Z","iopub.status.idle":"2022-02-14T12:11:36.682367Z","shell.execute_reply.started":"2022-02-14T12:11:36.677694Z","shell.execute_reply":"2022-02-14T12:11:36.68201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So here, for the same location we get two different values and that is for the replacement of the default values in **data**.\n\nNow we can check what are the total subcategories before and after manipulation.","metadata":{}},{"cell_type":"code","source":"len(df['joblocation_address'].value_counts().index)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:36.683142Z","iopub.execute_input":"2022-02-14T12:11:36.683687Z","iopub.status.idle":"2022-02-14T12:11:36.702809Z","shell.execute_reply.started":"2022-02-14T12:11:36.683664Z","shell.execute_reply":"2022-02-14T12:11:36.7019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can find this using the following code as well. ","metadata":{}},{"cell_type":"code","source":"data['joblocation_address'].nunique()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:36.704085Z","iopub.execute_input":"2022-02-14T12:11:36.704689Z","iopub.status.idle":"2022-02-14T12:11:36.722135Z","shell.execute_reply.started":"2022-02-14T12:11:36.70465Z","shell.execute_reply":"2022-02-14T12:11:36.720585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Removing unnecessary columns","metadata":{}},{"cell_type":"code","source":"def drop_feature(column):\n    data.drop(column, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:36.723913Z","iopub.execute_input":"2022-02-14T12:11:36.724085Z","iopub.status.idle":"2022-02-14T12:11:36.731637Z","shell.execute_reply.started":"2022-02-14T12:11:36.724062Z","shell.execute_reply":"2022-02-14T12:11:36.730981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_feature(['payrate', 'experience', 'postdate', 'uniq_id'])","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:36.733002Z","iopub.execute_input":"2022-02-14T12:11:36.733165Z","iopub.status.idle":"2022-02-14T12:11:36.75096Z","shell.execute_reply.started":"2022-02-14T12:11:36.733143Z","shell.execute_reply":"2022-02-14T12:11:36.750037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:36.752349Z","iopub.execute_input":"2022-02-14T12:11:36.752716Z","iopub.status.idle":"2022-02-14T12:11:36.759715Z","shell.execute_reply.started":"2022-02-14T12:11:36.752691Z","shell.execute_reply":"2022-02-14T12:11:36.759089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:11:36.761278Z","iopub.execute_input":"2022-02-14T12:11:36.761658Z","iopub.status.idle":"2022-02-14T12:11:36.789111Z","shell.execute_reply.started":"2022-02-14T12:11:36.761626Z","shell.execute_reply":"2022-02-14T12:11:36.787699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will convert this data into csv","metadata":{}},{"cell_type":"code","source":"data.to_csv('Job Market Analysis.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:13:50.520201Z","iopub.execute_input":"2022-02-14T12:13:50.520413Z","iopub.status.idle":"2022-02-14T12:13:51.446267Z","shell.execute_reply.started":"2022-02-14T12:13:50.520391Z","shell.execute_reply":"2022-02-14T12:13:51.445247Z"},"trusted":true},"execution_count":null,"outputs":[]}]}