{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:28.277719Z",
     "iopub.status.busy": "2022-02-14T12:11:28.277272Z",
     "iopub.status.idle": "2022-02-14T12:11:28.291227Z",
     "shell.execute_reply": "2022-02-14T12:11:28.290603Z",
     "shell.execute_reply.started": "2022-02-14T12:11:28.277692Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "\n",
    "\n",
    "## I have learned the processes by following a course on Udemy\n",
    "## The link of the course is: https://www.udemy.com/course/data-analytics-real-world-case-studies-using-python/?src=sac&kw=data+analytics+real+world\n",
    "## Since I have learned everything from that course, I decided to add it as my notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Data Science\\\\Projects\\\\Data Analysis\\\\Project 1 - Job Market Analysis\\\\Output'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Data Science\\Projects\\Data Analysis\\Project 1 - Job Market Analysis\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is Work\n",
      " Volume Serial Number is 8034-0675\n",
      "\n",
      " Directory of D:\\Data Science\\Projects\\Data Analysis\\Project 1 - Job Market Analysis\\Output\n",
      "\n",
      "02/19/2022  02:14 AM    <DIR>          .\n",
      "02/19/2022  02:14 AM    <DIR>          ..\n",
      "02/15/2022  10:48 PM    <DIR>          .ipynb_checkpoints\n",
      "02/19/2022  02:14 AM            30,160 Descriptive_stat_on_job_data.ipynb\n",
      "02/14/2022  06:14 PM        50,653,564 Job Market Analysis.csv\n",
      "               2 File(s)     50,683,724 bytes\n",
      "               3 Dir(s)  21,216,190,464 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:28.292722Z",
     "iopub.status.busy": "2022-02-14T12:11:28.292506Z",
     "iopub.status.idle": "2022-02-14T12:11:28.783599Z",
     "shell.execute_reply": "2022-02-14T12:11:28.78255Z",
     "shell.execute_reply.started": "2022-02-14T12:11:28.29269Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('job_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:28.785304Z",
     "iopub.status.busy": "2022-02-14T12:11:28.785117Z",
     "iopub.status.idle": "2022-02-14T12:11:28.801999Z",
     "shell.execute_reply": "2022-02-14T12:11:28.800642Z",
     "shell.execute_reply.started": "2022-02-14T12:11:28.785283Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>education</th>\n",
       "      <th>experience</th>\n",
       "      <th>industry</th>\n",
       "      <th>jobdescription</th>\n",
       "      <th>jobid</th>\n",
       "      <th>joblocation_address</th>\n",
       "      <th>jobtitle</th>\n",
       "      <th>numberofpositions</th>\n",
       "      <th>payrate</th>\n",
       "      <th>postdate</th>\n",
       "      <th>site_name</th>\n",
       "      <th>skills</th>\n",
       "      <th>uniq_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MM Media Pvt Ltd</td>\n",
       "      <td>UG: B.Tech/B.E. - Any Specialization PG:Any Po...</td>\n",
       "      <td>0 - 1 yrs</td>\n",
       "      <td>Media / Entertainment / Internet</td>\n",
       "      <td>Job Description   Send me Jobs like this Quali...</td>\n",
       "      <td>210516002263</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Walkin Data Entry Operator (night Shift)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1,50,000 - 2,25,000 P.A</td>\n",
       "      <td>2016-05-21 19:30:00 +0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ITES</td>\n",
       "      <td>43b19632647068535437c774b6ca6cf8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>find live infotech</td>\n",
       "      <td>UG: B.Tech/B.E. - Any Specialization PG:MBA/PG...</td>\n",
       "      <td>0 - 0 yrs</td>\n",
       "      <td>Advertising / PR / MR / Event Management</td>\n",
       "      <td>Job Description   Send me Jobs like this Quali...</td>\n",
       "      <td>210516002391</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Work Based Onhome Based Part Time.</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1,50,000 - 2,50,000 P.A. 20000</td>\n",
       "      <td>2016-05-21 19:30:00 +0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>d4c72325e57f89f364812b5ed5a795f0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Softtech Career Infosystem Pvt. Ltd</td>\n",
       "      <td>UG: Any Graduate - Any Specialization PG:Any P...</td>\n",
       "      <td>4 - 8 yrs</td>\n",
       "      <td>IT-Software / Software Services</td>\n",
       "      <td>Job Description   Send me Jobs like this - as ...</td>\n",
       "      <td>101016900534</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Pl/sql Developer - SQL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Disclosed by Recruiter</td>\n",
       "      <td>2016-10-13 16:20:55 +0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IT Software - Application Programming</td>\n",
       "      <td>c47df6f4cfdf5b46f1fd713ba61b9eba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Onboard HRServices LLP</td>\n",
       "      <td>UG: Any Graduate - Any Specialization PG:CA Do...</td>\n",
       "      <td>11 - 15 yrs</td>\n",
       "      <td>Banking / Financial Services / Broking</td>\n",
       "      <td>Job Description   Send me Jobs like this - Inv...</td>\n",
       "      <td>81016900536</td>\n",
       "      <td>Mumbai, Bengaluru, Kolkata, Chennai, Coimbator...</td>\n",
       "      <td>Manager/ad/partner - Indirect Tax - CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Disclosed by Recruiter</td>\n",
       "      <td>2016-10-13 16:20:55 +0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Accounts</td>\n",
       "      <td>115d28f140f694dd1cc61c53d03c66ae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spire Technologies and Solutions Pvt. Ltd.</td>\n",
       "      <td>UG: B.Tech/B.E. - Any Specialization PG:Any Po...</td>\n",
       "      <td>6 - 8 yrs</td>\n",
       "      <td>IT-Software / Software Services</td>\n",
       "      <td>Job Description   Send me Jobs like this Pleas...</td>\n",
       "      <td>120916002122</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>JAVA Technical Lead (6-8 yrs) -</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Not Disclosed by Recruiter</td>\n",
       "      <td>2016-10-13 16:20:55 +0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IT Software - Application Programming</td>\n",
       "      <td>a12553fc03bc7bcced8b1bb8963f97b4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      company  \\\n",
       "0                            MM Media Pvt Ltd   \n",
       "1                          find live infotech   \n",
       "2         Softtech Career Infosystem Pvt. Ltd   \n",
       "3                      Onboard HRServices LLP   \n",
       "4  Spire Technologies and Solutions Pvt. Ltd.   \n",
       "\n",
       "                                           education   experience  \\\n",
       "0  UG: B.Tech/B.E. - Any Specialization PG:Any Po...    0 - 1 yrs   \n",
       "1  UG: B.Tech/B.E. - Any Specialization PG:MBA/PG...    0 - 0 yrs   \n",
       "2  UG: Any Graduate - Any Specialization PG:Any P...    4 - 8 yrs   \n",
       "3  UG: Any Graduate - Any Specialization PG:CA Do...  11 - 15 yrs   \n",
       "4  UG: B.Tech/B.E. - Any Specialization PG:Any Po...    6 - 8 yrs   \n",
       "\n",
       "                                   industry  \\\n",
       "0          Media / Entertainment / Internet   \n",
       "1  Advertising / PR / MR / Event Management   \n",
       "2           IT-Software / Software Services   \n",
       "3    Banking / Financial Services / Broking   \n",
       "4           IT-Software / Software Services   \n",
       "\n",
       "                                      jobdescription         jobid  \\\n",
       "0  Job Description   Send me Jobs like this Quali...  210516002263   \n",
       "1  Job Description   Send me Jobs like this Quali...  210516002391   \n",
       "2  Job Description   Send me Jobs like this - as ...  101016900534   \n",
       "3  Job Description   Send me Jobs like this - Inv...   81016900536   \n",
       "4  Job Description   Send me Jobs like this Pleas...  120916002122   \n",
       "\n",
       "                                 joblocation_address  \\\n",
       "0                                            Chennai   \n",
       "1                                            Chennai   \n",
       "2                                          Bengaluru   \n",
       "3  Mumbai, Bengaluru, Kolkata, Chennai, Coimbator...   \n",
       "4                                          Bengaluru   \n",
       "\n",
       "                                   jobtitle  numberofpositions  \\\n",
       "0  Walkin Data Entry Operator (night Shift)                NaN   \n",
       "1        Work Based Onhome Based Part Time.               60.0   \n",
       "2                    Pl/sql Developer - SQL                NaN   \n",
       "3    Manager/ad/partner - Indirect Tax - CA                NaN   \n",
       "4           JAVA Technical Lead (6-8 yrs) -                4.0   \n",
       "\n",
       "                          payrate                   postdate site_name  \\\n",
       "0         1,50,000 - 2,25,000 P.A  2016-05-21 19:30:00 +0000       NaN   \n",
       "1  1,50,000 - 2,50,000 P.A. 20000  2016-05-21 19:30:00 +0000       NaN   \n",
       "2      Not Disclosed by Recruiter  2016-10-13 16:20:55 +0000       NaN   \n",
       "3      Not Disclosed by Recruiter  2016-10-13 16:20:55 +0000       NaN   \n",
       "4      Not Disclosed by Recruiter  2016-10-13 16:20:55 +0000       NaN   \n",
       "\n",
       "                                  skills                           uniq_id  \n",
       "0                                   ITES  43b19632647068535437c774b6ca6cf8  \n",
       "1                              Marketing  d4c72325e57f89f364812b5ed5a795f0  \n",
       "2  IT Software - Application Programming  c47df6f4cfdf5b46f1fd713ba61b9eba  \n",
       "3                               Accounts  115d28f140f694dd1cc61c53d03c66ae  \n",
       "4  IT Software - Application Programming  a12553fc03bc7bcced8b1bb8963f97b4  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:28.803388Z",
     "iopub.status.busy": "2022-02-14T12:11:28.8032Z",
     "iopub.status.idle": "2022-02-14T12:11:28.818218Z",
     "shell.execute_reply": "2022-02-14T12:11:28.81774Z",
     "shell.execute_reply.started": "2022-02-14T12:11:28.803365Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['company', 'education', 'experience', 'industry', 'jobdescription',\n",
       "       'jobid', 'joblocation_address', 'jobtitle', 'numberofpositions',\n",
       "       'payrate', 'postdate', 'site_name', 'skills', 'uniq_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:28.820156Z",
     "iopub.status.busy": "2022-02-14T12:11:28.819826Z",
     "iopub.status.idle": "2022-02-14T12:11:28.833369Z",
     "shell.execute_reply": "2022-02-14T12:11:28.832587Z",
     "shell.execute_reply.started": "2022-02-14T12:11:28.820133Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22000, 14)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:28.834706Z",
     "iopub.status.busy": "2022-02-14T12:11:28.834523Z",
     "iopub.status.idle": "2022-02-14T12:11:28.862437Z",
     "shell.execute_reply": "2022-02-14T12:11:28.86196Z",
     "shell.execute_reply.started": "2022-02-14T12:11:28.834684Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company                    4\n",
       "education               1996\n",
       "experience                 4\n",
       "industry                   5\n",
       "jobdescription             4\n",
       "jobid                      0\n",
       "joblocation_address      501\n",
       "jobtitle                   0\n",
       "numberofpositions      17536\n",
       "payrate                   97\n",
       "postdate                  23\n",
       "site_name              18013\n",
       "skills                   528\n",
       "uniq_id                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_missing = df.isnull().sum()\n",
    "count_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:28.86455Z",
     "iopub.status.busy": "2022-02-14T12:11:28.863709Z",
     "iopub.status.idle": "2022-02-14T12:11:28.906741Z",
     "shell.execute_reply": "2022-02-14T12:11:28.905965Z",
     "shell.execute_reply.started": "2022-02-14T12:11:28.864514Z"
    }
   },
   "outputs": [],
   "source": [
    "percent_missing = (df.isnull().sum())* (100/(len(df)))\n",
    "percent_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making a dataframe from the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:28.907992Z",
     "iopub.status.busy": "2022-02-14T12:11:28.907795Z",
     "iopub.status.idle": "2022-02-14T12:11:28.913552Z",
     "shell.execute_reply": "2022-02-14T12:11:28.912693Z",
     "shell.execute_reply.started": "2022-02-14T12:11:28.907965Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_value_df = pd.DataFrame({\"count_missing\": count_missing,\n",
    "                                 \"percent_missing\": percent_missing})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:28.915615Z",
     "iopub.status.busy": "2022-02-14T12:11:28.915147Z",
     "iopub.status.idle": "2022-02-14T12:11:28.940487Z",
     "shell.execute_reply": "2022-02-14T12:11:28.939815Z",
     "shell.execute_reply.started": "2022-02-14T12:11:28.915576Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_value_df.style.background_gradient(cmap='Spectral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return unique values of all the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:28.942473Z",
     "iopub.status.busy": "2022-02-14T12:11:28.94151Z",
     "iopub.status.idle": "2022-02-14T12:11:29.018017Z",
     "shell.execute_reply": "2022-02-14T12:11:29.016764Z",
     "shell.execute_reply.started": "2022-02-14T12:11:28.94244Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(f\"{col} has {df[col].nunique()} values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list containing row name, number of unique values and the unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.020528Z",
     "iopub.status.busy": "2022-02-14T12:11:29.020271Z",
     "iopub.status.idle": "2022-02-14T12:11:29.08292Z",
     "shell.execute_reply": "2022-02-14T12:11:29.08185Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.020492Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_df = [[col, df[col].nunique(), df[col].unique()] for col in df.columns]\n",
    "unique_df[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We used list comprehension here. The code above works exactly like the following:\n",
    "    \n",
    "<code>\n",
    "unique = []\n",
    "for col in df.columns:\n",
    "    unique.append([col, df[col].nunique(), df[col].unique()])\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.084557Z",
     "iopub.status.busy": "2022-02-14T12:11:29.084352Z",
     "iopub.status.idle": "2022-02-14T12:11:29.099339Z",
     "shell.execute_reply": "2022-02-14T12:11:29.09894Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.084535Z"
    }
   },
   "outputs": [],
   "source": [
    "count_df = pd.DataFrame(unique_df, columns=['col_name', 'count', 'unique'])\n",
    "count_df.style.background_gradient(cmap='Spectral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the 'payrate' feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.100512Z",
     "iopub.status.busy": "2022-02-14T12:11:29.100249Z",
     "iopub.status.idle": "2022-02-14T12:11:29.124574Z",
     "shell.execute_reply": "2022-02-14T12:11:29.124079Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.100479Z"
    }
   },
   "outputs": [],
   "source": [
    "df['payrate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will find the following for each row<br>\n",
    "1. Minimum Payrate<br>\n",
    "2. Maximum Payrate<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.125672Z",
     "iopub.status.busy": "2022-02-14T12:11:29.125427Z",
     "iopub.status.idle": "2022-02-14T12:11:29.137262Z",
     "shell.execute_reply": "2022-02-14T12:11:29.136624Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.125643Z"
    }
   },
   "outputs": [],
   "source": [
    "# The minimum and maximum payrate\n",
    "df['payrate'][0].split(' - ')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.13858Z",
     "iopub.status.busy": "2022-02-14T12:11:29.138258Z",
     "iopub.status.idle": "2022-02-14T12:11:29.153857Z",
     "shell.execute_reply": "2022-02-14T12:11:29.153028Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.138554Z"
    }
   },
   "outputs": [],
   "source": [
    "min_and_max = df['payrate'][0].split(' - ')\n",
    "# min_and_max =>  Gives output ['1,50,000', '2,25,000 P.A']\n",
    "\n",
    "# We will remove ' P.A' from the second i.e. the maximum payrate, so that we get two numbers from min_and_max variable \n",
    "min_and_max[1] = min_and_max[1].split(' ')[0]   # This gives '2,25,000'\n",
    "\n",
    "min_and_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have done above only applies for those cells that have a minimum and maximum range. Other than that, the rest will have to be processed diffrently.<br>\n",
    "Here we can say that, after <i>splitting</i>, if there are two values in the list, the values will surely be minimum and maximum payrate (exactly like <i><u>min_and_max</u></i>). So the length of those valid cells will be 2.<br>\n",
    "We can easily find how many valid cells are there using the following method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.155674Z",
     "iopub.status.busy": "2022-02-14T12:11:29.15541Z",
     "iopub.status.idle": "2022-02-14T12:11:29.177257Z",
     "shell.execute_reply": "2022-02-14T12:11:29.176233Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.155639Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We are creating a list that will have the lengths of the lists after splitting the string around '-'\n",
    "'''\n",
    "len_pay = []\n",
    "for pay in df['payrate']:\n",
    "    len_pay.append(len(str(pay).split('-')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.179135Z",
     "iopub.status.busy": "2022-02-14T12:11:29.17883Z",
     "iopub.status.idle": "2022-02-14T12:11:29.198018Z",
     "shell.execute_reply": "2022-02-14T12:11:29.197253Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.179103Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(len_pay).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 4682 values that are valid entries for such processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will create a feature table that will hold all the ranges.<br>\n",
    "For example, if we have a payrate like 1k-2k-3k-4k, we have to store 1k, 2k, 3k, 4k in different columns.<br>\n",
    "This will be an optimal solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.199739Z",
     "iopub.status.busy": "2022-02-14T12:11:29.199078Z",
     "iopub.status.idle": "2022-02-14T12:11:29.250081Z",
     "shell.execute_reply": "2022-02-14T12:11:29.249307Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.19966Z"
    }
   },
   "outputs": [],
   "source": [
    "payrate_split = df['payrate'].str.split('-', expand=True)   \n",
    "\n",
    "'''\n",
    "By default the value of expand is False\n",
    "\n",
    "The expand parameter will expand the values in different columns as following\n",
    "'''\n",
    "\n",
    "payrate_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To properly convert the entries into integers, we have to: <br>\n",
    "1. Remove extra spaces<br>\n",
    "2. Remove commas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.251165Z",
     "iopub.status.busy": "2022-02-14T12:11:29.251005Z",
     "iopub.status.idle": "2022-02-14T12:11:29.256728Z",
     "shell.execute_reply": "2022-02-14T12:11:29.256271Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.251135Z"
    }
   },
   "outputs": [],
   "source": [
    "payrate_split[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.258059Z",
     "iopub.status.busy": "2022-02-14T12:11:29.257912Z",
     "iopub.status.idle": "2022-02-14T12:11:29.276178Z",
     "shell.execute_reply": "2022-02-14T12:11:29.275401Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.258038Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "payrate_split[0] = payrate_split[0].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.277246Z",
     "iopub.status.busy": "2022-02-14T12:11:29.277084Z",
     "iopub.status.idle": "2022-02-14T12:11:29.301494Z",
     "shell.execute_reply": "2022-02-14T12:11:29.301119Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.277223Z"
    }
   },
   "outputs": [],
   "source": [
    "payrate_split[0].str.replace(',', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use <b>Lambda</b> function to replace comma will an empty string for each entry<br>\n",
    "It will do the same as the previous cell did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.302624Z",
     "iopub.status.busy": "2022-02-14T12:11:29.30239Z",
     "iopub.status.idle": "2022-02-14T12:11:29.323598Z",
     "shell.execute_reply": "2022-02-14T12:11:29.323123Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.302599Z"
    }
   },
   "outputs": [],
   "source": [
    "payrate_split[0] = payrate_split[0].apply(lambda x: str(x).replace(',', ''))\n",
    "\n",
    "payrate_split[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to find whether there are any float values or not.<br>\n",
    "Also we have to deal with the string values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T18:36:01.581309Z",
     "iopub.status.busy": "2022-01-24T18:36:01.581082Z",
     "iopub.status.idle": "2022-01-24T18:36:01.585685Z",
     "shell.execute_reply": "2022-01-24T18:36:01.584556Z",
     "shell.execute_reply.started": "2022-01-24T18:36:01.58128Z"
    }
   },
   "source": [
    "Now I will get the <b>minimum pay</b> if there is float or int values<br>\n",
    "We have to extract or seperate the values from the features.<br>\n",
    "Possible ways:<br>\n",
    "1. Exception Handling/ try except block\n",
    "2. Regular expression\n",
    "3. <i>to_numeric()</i> function in Pandas\n",
    "4. Any function along with <i>map</i> function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Exception\n",
    "\n",
    "> In the <i>try block</i>, there will be an <b>if condition</b> that will check whether the value is float. If not, then that is the missing value\n",
    "\n",
    "<br>\n",
    "\n",
    "> try:<br>\n",
    "> &emsp; if dtype(payrate) == float<br>\n",
    "> except:<br>\n",
    "> &emsp; \"Missing Value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.32485Z",
     "iopub.status.busy": "2022-02-14T12:11:29.324565Z",
     "iopub.status.idle": "2022-02-14T12:11:29.36752Z",
     "shell.execute_reply": "2022-02-14T12:11:29.367063Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.324814Z"
    }
   },
   "outputs": [],
   "source": [
    "pay = []      # This will contain the converted values of the first column of \"payrate_split\"\n",
    "for payrate in payrate_split[0]:\n",
    "    try:\n",
    "        if type(float(payrate)) == np.float:\n",
    "            pay.append(payrate)\n",
    "    except:\n",
    "            pay.append(\"missing value\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. to_numeric function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.368655Z",
     "iopub.status.busy": "2022-02-14T12:11:29.368471Z",
     "iopub.status.idle": "2022-02-14T12:11:29.392032Z",
     "shell.execute_reply": "2022-02-14T12:11:29.391283Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.368627Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.to_numeric(payrate_split[0], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Regular Expression\n",
    "\n",
    "> pattern = '\\D.*' <br>\n",
    "> This means anything other than digits\n",
    "\n",
    "So if I get anything other than digits, I will replace it will an empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.393859Z",
     "iopub.status.busy": "2022-02-14T12:11:29.393579Z",
     "iopub.status.idle": "2022-02-14T12:11:29.403337Z",
     "shell.execute_reply": "2022-02-14T12:11:29.402811Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.393825Z"
    }
   },
   "outputs": [],
   "source": [
    "# A pattern that contains no numeric values\n",
    "pattern = '\\D.*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.405397Z",
     "iopub.status.busy": "2022-02-14T12:11:29.404233Z",
     "iopub.status.idle": "2022-02-14T12:11:29.439887Z",
     "shell.execute_reply": "2022-02-14T12:11:29.43921Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.405339Z"
    }
   },
   "outputs": [],
   "source": [
    "payrate_split[0].str.replace(pattern, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. \"any\" and \"map\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.440987Z",
     "iopub.status.busy": "2022-02-14T12:11:29.440802Z",
     "iopub.status.idle": "2022-02-14T12:11:29.448815Z",
     "shell.execute_reply": "2022-02-14T12:11:29.447907Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.440963Z"
    }
   },
   "outputs": [],
   "source": [
    "any(map(str.isdigit, payrate_split[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.450437Z",
     "iopub.status.busy": "2022-02-14T12:11:29.449878Z",
     "iopub.status.idle": "2022-02-14T12:11:29.461632Z",
     "shell.execute_reply": "2022-02-14T12:11:29.461165Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.450404Z"
    }
   },
   "outputs": [],
   "source": [
    "any(map(str.isnumeric, payrate_split[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will look and do something with the remaining columns of <i>payrate_split</i> <br>\n",
    "The second column, or <b>payrate_split[1]</b> will have the max value. Here we have to follow some steps, which are: <br>\n",
    "1. Removing all the extra white spaces.\n",
    "2. Removing all the commas.\n",
    "3. Removing set of characters.\n",
    "4. Changing data type to float/int.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.465977Z",
     "iopub.status.busy": "2022-02-14T12:11:29.465699Z",
     "iopub.status.idle": "2022-02-14T12:11:29.481383Z",
     "shell.execute_reply": "2022-02-14T12:11:29.480187Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.465955Z"
    }
   },
   "outputs": [],
   "source": [
    "payrate_split[1] = payrate_split[1].str.strip()\n",
    "payrate_split[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.483118Z",
     "iopub.status.busy": "2022-02-14T12:11:29.482597Z",
     "iopub.status.idle": "2022-02-14T12:11:29.503156Z",
     "shell.execute_reply": "2022-02-14T12:11:29.502111Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.483083Z"
    }
   },
   "outputs": [],
   "source": [
    "payrate_split[1] = payrate_split[1].apply(lambda x: str(x).replace(',', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.504466Z",
     "iopub.status.busy": "2022-02-14T12:11:29.504266Z",
     "iopub.status.idle": "2022-02-14T12:11:29.536807Z",
     "shell.execute_reply": "2022-02-14T12:11:29.536375Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.504443Z"
    }
   },
   "outputs": [],
   "source": [
    "pattern = '\\D.*'\n",
    "\n",
    "payrate_split[1] = payrate_split[1].str.replace(pattern, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.537858Z",
     "iopub.status.busy": "2022-02-14T12:11:29.537661Z",
     "iopub.status.idle": "2022-02-14T12:11:29.543993Z",
     "shell.execute_reply": "2022-02-14T12:11:29.543136Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.537829Z"
    }
   },
   "outputs": [],
   "source": [
    "payrate_split.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.545105Z",
     "iopub.status.busy": "2022-02-14T12:11:29.544927Z",
     "iopub.status.idle": "2022-02-14T12:11:29.58028Z",
     "shell.execute_reply": "2022-02-14T12:11:29.57974Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.545082Z"
    }
   },
   "outputs": [],
   "source": [
    "payrate_split[0] = pd.to_numeric(payrate_split[0], errors='coerce')  # For the minimum pay feature\n",
    "payrate_split[1] = pd.to_numeric(payrate_split[1], errors='coerce')  # For the maximum pay feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.581358Z",
     "iopub.status.busy": "2022-02-14T12:11:29.581118Z",
     "iopub.status.idle": "2022-02-14T12:11:29.586399Z",
     "shell.execute_reply": "2022-02-14T12:11:29.58554Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.581336Z"
    }
   },
   "outputs": [],
   "source": [
    "payrate_split.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now we have to insert this minimum and maximum payrate in the main <i>df</i> dataframe which can be done in two ways. \n",
    "\n",
    "1. Defining new column in *df* for minimum and maximum payrate. (When you are adding below 5 features, just like now)\n",
    "2. Concatenation Approach, using pandas library. (When you are adding more than 5-10 features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.588589Z",
     "iopub.status.busy": "2022-02-14T12:11:29.587781Z",
     "iopub.status.idle": "2022-02-14T12:11:29.60017Z",
     "shell.execute_reply": "2022-02-14T12:11:29.599335Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.588547Z"
    }
   },
   "outputs": [],
   "source": [
    "# df['min_pay'] = payrate_split[0]\n",
    "# df['max_pay'] = payrate_split[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation using *pd.concat()* function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.601884Z",
     "iopub.status.busy": "2022-02-14T12:11:29.601123Z",
     "iopub.status.idle": "2022-02-14T12:11:29.622874Z",
     "shell.execute_reply": "2022-02-14T12:11:29.621802Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.601843Z"
    }
   },
   "outputs": [],
   "source": [
    "pay = pd.concat([payrate_split[0], payrate_split[1]], axis=1, sort=False)\n",
    "pay.columns = ['min_pay', 'max_pay']\n",
    "pay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.624504Z",
     "iopub.status.busy": "2022-02-14T12:11:29.623989Z",
     "iopub.status.idle": "2022-02-14T12:11:29.633686Z",
     "shell.execute_reply": "2022-02-14T12:11:29.633182Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.624468Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df, pay], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.634907Z",
     "iopub.status.busy": "2022-02-14T12:11:29.634457Z",
     "iopub.status.idle": "2022-02-14T12:11:29.658135Z",
     "shell.execute_reply": "2022-02-14T12:11:29.657588Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.63488Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and featurizing *'experience'*  feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.659439Z",
     "iopub.status.busy": "2022-02-14T12:11:29.659142Z",
     "iopub.status.idle": "2022-02-14T12:11:29.664575Z",
     "shell.execute_reply": "2022-02-14T12:11:29.663923Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.659414Z"
    }
   },
   "outputs": [],
   "source": [
    "df['experience'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.666042Z",
     "iopub.status.busy": "2022-02-14T12:11:29.665503Z",
     "iopub.status.idle": "2022-02-14T12:11:29.679298Z",
     "shell.execute_reply": "2022-02-14T12:11:29.678546Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.666016Z"
    }
   },
   "outputs": [],
   "source": [
    "# exp_list = list(df['experience'].apply(lambda x: str(x)[:-3:].strip().split(' - ')))\n",
    "\n",
    "# exp_len = [len(i) for i in exp_list]\n",
    "# # exp_list, ex_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T17:45:10.271984Z",
     "iopub.status.busy": "2022-02-11T17:45:10.271474Z",
     "iopub.status.idle": "2022-02-11T17:45:10.276742Z",
     "shell.execute_reply": "2022-02-11T17:45:10.276055Z",
     "shell.execute_reply.started": "2022-02-11T17:45:10.271949Z"
    }
   },
   "source": [
    "    def split_exp(exp):    \n",
    "        min_exp = exp.split('-')[0]\n",
    "        max_exp = exp.split('-')[1]\n",
    "    \n",
    "        return min_exp, max_exp\n",
    "        \n",
    "        \n",
    "  We could have done the above. But we won't always have sweet data as '1 - 3 year', we may have **null** values or **1 - 3 - 5 years** type values. <br>\n",
    "  So what we have to do here is as following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.680687Z",
     "iopub.status.busy": "2022-02-14T12:11:29.680255Z",
     "iopub.status.idle": "2022-02-14T12:11:29.703247Z",
     "shell.execute_reply": "2022-02-14T12:11:29.702807Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.680644Z"
    }
   },
   "outputs": [],
   "source": [
    "len1 = []\n",
    "\n",
    "for exp in df['experience'].dropna():\n",
    "    if len(exp.split('-')) != 2:\n",
    "        len1.append(exp)\n",
    "        \n",
    "len1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I can drop all the rows having \"Not Mentioned\", which is not a handy approach. Other than this, there is also an optimal solution which is enhancing split_exp function by using \"exception handling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.704421Z",
     "iopub.status.busy": "2022-02-14T12:11:29.704248Z",
     "iopub.status.idle": "2022-02-14T12:11:29.709568Z",
     "shell.execute_reply": "2022-02-14T12:11:29.708735Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.704392Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_exp(exp):\n",
    "    try:\n",
    "        if len(exp.split('-')) == 2:\n",
    "            min_exp = exp.split('-')[0]\n",
    "            max_exp = exp.split('-')[1]\n",
    "        return pd.Series([min_exp, max_exp])\n",
    "    except:\n",
    "        return pd.Series([np.nan, np.nan])\n",
    "    \n",
    "# To append the new data easily, we are returning pd.Series() data here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:29.711212Z",
     "iopub.status.busy": "2022-02-14T12:11:29.710822Z",
     "iopub.status.idle": "2022-02-14T12:11:35.148985Z",
     "shell.execute_reply": "2022-02-14T12:11:35.148097Z",
     "shell.execute_reply.started": "2022-02-14T12:11:29.711186Z"
    }
   },
   "outputs": [],
   "source": [
    "df[['min_exp', 'max_exp']] = df['experience'].apply(split_exp).rename(columns={0:'min_exp', 1:'max_exp'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will talk about the rows where the experience value is **Not Mentioned**<br>\n",
    "I will create another DataFrame for that and filter out those peculiar values.<br>\n",
    "We will store those values in *nm* variable which means *not mentioned*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:35.150389Z",
     "iopub.status.busy": "2022-02-14T12:11:35.150131Z",
     "iopub.status.idle": "2022-02-14T12:11:35.1741Z",
     "shell.execute_reply": "2022-02-14T12:11:35.172933Z",
     "shell.execute_reply.started": "2022-02-14T12:11:35.150352Z"
    }
   },
   "outputs": [],
   "source": [
    "nm = pd.DataFrame(df['experience'].str.contains(\"Not Mentioned\"))\n",
    "\n",
    "nm[nm['experience'] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if I want to get the indices of these values, then I will add *.index* after the previous line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:35.175505Z",
     "iopub.status.busy": "2022-02-14T12:11:35.175322Z",
     "iopub.status.idle": "2022-02-14T12:11:35.192639Z",
     "shell.execute_reply": "2022-02-14T12:11:35.191897Z",
     "shell.execute_reply.started": "2022-02-14T12:11:35.175482Z"
    }
   },
   "outputs": [],
   "source": [
    "nm[nm['experience'] == True].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above values are indices of those rows with values \"Not Mentioned\".<br>\n",
    "Now if I want to put this value into the funtion **split_exp**, I will get NaN values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:35.193788Z",
     "iopub.status.busy": "2022-02-14T12:11:35.193585Z",
     "iopub.status.idle": "2022-02-14T12:11:35.202055Z",
     "shell.execute_reply": "2022-02-14T12:11:35.201656Z",
     "shell.execute_reply.started": "2022-02-14T12:11:35.193735Z"
    }
   },
   "outputs": [],
   "source": [
    "nm['experience'][1138]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:35.203522Z",
     "iopub.status.busy": "2022-02-14T12:11:35.202742Z",
     "iopub.status.idle": "2022-02-14T12:11:35.215611Z",
     "shell.execute_reply": "2022-02-14T12:11:35.215232Z",
     "shell.execute_reply.started": "2022-02-14T12:11:35.203497Z"
    }
   },
   "outputs": [],
   "source": [
    "# The result of this code will be NaN. Because the returned values causes an exception.\n",
    "split_exp(df['experience'][1138])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:35.218018Z",
     "iopub.status.busy": "2022-02-14T12:11:35.217504Z",
     "iopub.status.idle": "2022-02-14T12:11:35.244771Z",
     "shell.execute_reply": "2022-02-14T12:11:35.243801Z",
     "shell.execute_reply.started": "2022-02-14T12:11:35.217978Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now removing *'yrs'* from the **max_exp** column and update the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:35.24671Z",
     "iopub.status.busy": "2022-02-14T12:11:35.246252Z",
     "iopub.status.idle": "2022-02-14T12:11:35.276615Z",
     "shell.execute_reply": "2022-02-14T12:11:35.276144Z",
     "shell.execute_reply.started": "2022-02-14T12:11:35.246677Z"
    }
   },
   "outputs": [],
   "source": [
    "df['max_exp'] = df['max_exp'].str.replace('yrs', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:35.278514Z",
     "iopub.status.busy": "2022-02-14T12:11:35.277637Z",
     "iopub.status.idle": "2022-02-14T12:11:35.29944Z",
     "shell.execute_reply": "2022-02-14T12:11:35.298743Z",
     "shell.execute_reply.started": "2022-02-14T12:11:35.278447Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:35.300943Z",
     "iopub.status.busy": "2022-02-14T12:11:35.300715Z",
     "iopub.status.idle": "2022-02-14T12:11:35.309371Z",
     "shell.execute_reply": "2022-02-14T12:11:35.308405Z",
     "shell.execute_reply.started": "2022-02-14T12:11:35.300915Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will convert min_exp and max_exp to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:35.311237Z",
     "iopub.status.busy": "2022-02-14T12:11:35.31102Z",
     "iopub.status.idle": "2022-02-14T12:11:35.329673Z",
     "shell.execute_reply": "2022-02-14T12:11:35.328913Z",
     "shell.execute_reply.started": "2022-02-14T12:11:35.311208Z"
    }
   },
   "outputs": [],
   "source": [
    "df['max_exp'] = df['max_exp'].astype(float)\n",
    "df['min_exp'] = df['min_exp'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:35.331553Z",
     "iopub.status.busy": "2022-02-14T12:11:35.331195Z",
     "iopub.status.idle": "2022-02-14T12:11:35.339633Z",
     "shell.execute_reply": "2022-02-14T12:11:35.338662Z",
     "shell.execute_reply.started": "2022-02-14T12:11:35.331527Z"
    }
   },
   "outputs": [],
   "source": [
    "df['avg_experience'] = (df['min_exp'] + df['max_exp'])/2\n",
    "df['avg_payrate'] = (df['min_pay'] + df['max_pay'])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the **min_pay, max_pay** columns and **min_exp, max_exp** columns, avg_pay and avg_exp can be derived. Which will give us more insight on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Feature Engineering on **postdate** feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approaches:\n",
    "1. Define own function.\n",
    "2. Inbuilt functions, using datetime module\n",
    "3. Optimal way: Lambda function\n",
    "4. map function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 1. Define own function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:35.341084Z",
     "iopub.status.busy": "2022-02-14T12:11:35.340878Z",
     "iopub.status.idle": "2022-02-14T12:11:35.353905Z",
     "shell.execute_reply": "2022-02-14T12:11:35.352992Z",
     "shell.execute_reply.started": "2022-02-14T12:11:35.341059Z"
    }
   },
   "outputs": [],
   "source": [
    "df['postdate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 types of datetime datatypes:\n",
    "1. datetime64[ns]\n",
    "2. <M8[ns]\n",
    "\n",
    "both are basically same. But the entire structure depends on how numpy datatype is designed. <br>\n",
    "In the following code we can see that both the datatypes are same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:35.35549Z",
     "iopub.status.busy": "2022-02-14T12:11:35.355128Z",
     "iopub.status.idle": "2022-02-14T12:11:35.364731Z",
     "shell.execute_reply": "2022-02-14T12:11:35.36434Z",
     "shell.execute_reply.started": "2022-02-14T12:11:35.355458Z"
    }
   },
   "outputs": [],
   "source": [
    "np.dtype('datetime64[ns]') == np.dtype('<M8[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:35.366392Z",
     "iopub.status.busy": "2022-02-14T12:11:35.366015Z",
     "iopub.status.idle": "2022-02-14T12:11:35.378859Z",
     "shell.execute_reply": "2022-02-14T12:11:35.37801Z",
     "shell.execute_reply.started": "2022-02-14T12:11:35.366366Z"
    }
   },
   "outputs": [],
   "source": [
    "df['postdate'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will write a function that will get day, month and year from each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:35.380609Z",
     "iopub.status.busy": "2022-02-14T12:11:35.380265Z",
     "iopub.status.idle": "2022-02-14T12:11:35.388792Z",
     "shell.execute_reply": "2022-02-14T12:11:35.388114Z",
     "shell.execute_reply.started": "2022-02-14T12:11:35.380576Z"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_dt_att(dataframe, feature):\n",
    "    try:\n",
    "       return pd.Series([dataframe[feature].dt.day, dataframe[feature].dt.month, dataframe[feature].dt.year])\n",
    "    except:\n",
    "        print('Data type is not supported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:35.390308Z",
     "iopub.status.busy": "2022-02-14T12:11:35.389989Z",
     "iopub.status.idle": "2022-02-14T12:11:35.40659Z",
     "shell.execute_reply": "2022-02-14T12:11:35.405359Z",
     "shell.execute_reply.started": "2022-02-14T12:11:35.390276Z"
    }
   },
   "outputs": [],
   "source": [
    "fetch_dt_att(df, 'postdate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have not changed the data-type of the **postdate** feature. We will do it here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:35.409364Z",
     "iopub.status.busy": "2022-02-14T12:11:35.408774Z",
     "iopub.status.idle": "2022-02-14T12:11:35.430296Z",
     "shell.execute_reply": "2022-02-14T12:11:35.429214Z",
     "shell.execute_reply.started": "2022-02-14T12:11:35.409336Z"
    }
   },
   "outputs": [],
   "source": [
    "df['postdate'] = pd.to_datetime(df['postdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:35.43189Z",
     "iopub.status.busy": "2022-02-14T12:11:35.431568Z",
     "iopub.status.idle": "2022-02-14T12:11:35.462543Z",
     "shell.execute_reply": "2022-02-14T12:11:35.46181Z",
     "shell.execute_reply.started": "2022-02-14T12:11:35.431865Z"
    }
   },
   "outputs": [],
   "source": [
    "df[['day', 'month', 'year']] = fetch_dt_att(df, 'postdate')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 3. Using 'Lambda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 4. Use 'map' function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I will define the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:35.464241Z",
     "iopub.status.busy": "2022-02-14T12:11:35.463686Z",
     "iopub.status.idle": "2022-02-14T12:11:35.469517Z",
     "shell.execute_reply": "2022-02-14T12:11:35.467626Z",
     "shell.execute_reply.started": "2022-02-14T12:11:35.464203Z"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_dt_att2(x):\n",
    "    return ([x.day, x.month, x.year])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:35.471145Z",
     "iopub.status.busy": "2022-02-14T12:11:35.470731Z",
     "iopub.status.idle": "2022-02-14T12:11:35.538595Z",
     "shell.execute_reply": "2022-02-14T12:11:35.537899Z",
     "shell.execute_reply.started": "2022-02-14T12:11:35.471117Z"
    }
   },
   "outputs": [],
   "source": [
    "fe_date = pd.DataFrame(map(fetch_dt_att2, df['postdate'])).rename(columns={0: 'day', 1: 'month', 2: 'year'})\n",
    "fe_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can concatenate this **fe_date** with df and will get the 3 columns. Since we have already done that using the previous steps, here I will not do it.<br>\n",
    "Following is the code for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:35.540146Z",
     "iopub.status.busy": "2022-02-14T12:11:35.539544Z",
     "iopub.status.idle": "2022-02-14T12:11:35.543956Z",
     "shell.execute_reply": "2022-02-14T12:11:35.543432Z",
     "shell.execute_reply.started": "2022-02-14T12:11:35.540117Z"
    }
   },
   "outputs": [],
   "source": [
    "# pd.concat([df, fe_date], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare job_location feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:35.545315Z",
     "iopub.status.busy": "2022-02-14T12:11:35.545149Z",
     "iopub.status.idle": "2022-02-14T12:11:35.563708Z",
     "shell.execute_reply": "2022-02-14T12:11:35.563108Z",
     "shell.execute_reply.started": "2022-02-14T12:11:35.545292Z"
    }
   },
   "outputs": [],
   "source": [
    "df['joblocation_address'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a copy so that whatever manipulation is done, can be reverted back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:35.566543Z",
     "iopub.status.busy": "2022-02-14T12:11:35.565118Z",
     "iopub.status.idle": "2022-02-14T12:11:35.582191Z",
     "shell.execute_reply": "2022-02-14T12:11:35.581421Z",
     "shell.execute_reply.started": "2022-02-14T12:11:35.566476Z"
    }
   },
   "outputs": [],
   "source": [
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:35.583787Z",
     "iopub.status.busy": "2022-02-14T12:11:35.583456Z",
     "iopub.status.idle": "2022-02-14T12:11:35.59207Z",
     "shell.execute_reply": "2022-02-14T12:11:35.591607Z",
     "shell.execute_reply.started": "2022-02-14T12:11:35.583732Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(data['joblocation_address'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:35.593314Z",
     "iopub.status.busy": "2022-02-14T12:11:35.592817Z",
     "iopub.status.idle": "2022-02-14T12:11:35.608572Z",
     "shell.execute_reply": "2022-02-14T12:11:35.607728Z",
     "shell.execute_reply.started": "2022-02-14T12:11:35.593283Z"
    }
   },
   "outputs": [],
   "source": [
    "data['joblocation_address'].value_counts().head(60).index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, there are some data repeatations. <br>\n",
    "Some replacements are needed, which will be stored in dictionary. <br>\n",
    "I will upload another csv file that will have the replacements.<br>\n",
    "After this processing, the **duplicate subcategories** will mostly be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:35.61085Z",
     "iopub.status.busy": "2022-02-14T12:11:35.610569Z",
     "iopub.status.idle": "2022-02-14T12:11:35.63018Z",
     "shell.execute_reply": "2022-02-14T12:11:35.629117Z",
     "shell.execute_reply.started": "2022-02-14T12:11:35.610819Z"
    }
   },
   "outputs": [],
   "source": [
    "rep = pd.read_csv(r'/kaggle/input/d/junaidmahmud/replacements/replacements.csv').set_index('Unnamed: 0')\n",
    "rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:35.632198Z",
     "iopub.status.busy": "2022-02-14T12:11:35.63143Z",
     "iopub.status.idle": "2022-02-14T12:11:35.636844Z",
     "shell.execute_reply": "2022-02-14T12:11:35.63543Z",
     "shell.execute_reply.started": "2022-02-14T12:11:35.632158Z"
    }
   },
   "outputs": [],
   "source": [
    "replacement_dict = rep.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:35.638437Z",
     "iopub.status.busy": "2022-02-14T12:11:35.638208Z",
     "iopub.status.idle": "2022-02-14T12:11:36.606547Z",
     "shell.execute_reply": "2022-02-14T12:11:36.605886Z",
     "shell.execute_reply.started": "2022-02-14T12:11:35.638407Z"
    }
   },
   "outputs": [],
   "source": [
    "data.replace(replacement_dict, inplace=True, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:36.608105Z",
     "iopub.status.busy": "2022-02-14T12:11:36.607882Z",
     "iopub.status.idle": "2022-02-14T12:11:36.619735Z",
     "shell.execute_reply": "2022-02-14T12:11:36.618813Z",
     "shell.execute_reply.started": "2022-02-14T12:11:36.608075Z"
    }
   },
   "outputs": [],
   "source": [
    "data['joblocation_address'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:36.621878Z",
     "iopub.status.busy": "2022-02-14T12:11:36.621098Z",
     "iopub.status.idle": "2022-02-14T12:11:36.633692Z",
     "shell.execute_reply": "2022-02-14T12:11:36.632926Z",
     "shell.execute_reply.started": "2022-02-14T12:11:36.621845Z"
    }
   },
   "outputs": [],
   "source": [
    "df['joblocation_address'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now I will get some data using city names\n",
    "\n",
    "That can be done using the following ways:\n",
    "1. filter\n",
    "2. Query\n",
    "3. isin function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 1. Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:36.634969Z",
     "iopub.status.busy": "2022-02-14T12:11:36.634714Z",
     "iopub.status.idle": "2022-02-14T12:11:36.676385Z",
     "shell.execute_reply": "2022-02-14T12:11:36.675467Z",
     "shell.execute_reply.started": "2022-02-14T12:11:36.634945Z"
    }
   },
   "outputs": [],
   "source": [
    "loc_filter = df['joblocation_address'] == 'Noida'\n",
    "loc_noida = df[loc_filter]\n",
    "\n",
    "loc_noida.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:36.677716Z",
     "iopub.status.busy": "2022-02-14T12:11:36.67754Z",
     "iopub.status.idle": "2022-02-14T12:11:36.682367Z",
     "shell.execute_reply": "2022-02-14T12:11:36.68201Z",
     "shell.execute_reply.started": "2022-02-14T12:11:36.677694Z"
    }
   },
   "outputs": [],
   "source": [
    "df['joblocation_address'][1760], data['joblocation_address'][1760]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here, for the same location we get two different values and that is for the replacement of the default values in **data**.\n",
    "\n",
    "Now we can check what are the total subcategories before and after manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:36.683687Z",
     "iopub.status.busy": "2022-02-14T12:11:36.683142Z",
     "iopub.status.idle": "2022-02-14T12:11:36.702809Z",
     "shell.execute_reply": "2022-02-14T12:11:36.7019Z",
     "shell.execute_reply.started": "2022-02-14T12:11:36.683664Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df['joblocation_address'].value_counts().index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find this using the following code as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:36.704689Z",
     "iopub.status.busy": "2022-02-14T12:11:36.704085Z",
     "iopub.status.idle": "2022-02-14T12:11:36.722135Z",
     "shell.execute_reply": "2022-02-14T12:11:36.720585Z",
     "shell.execute_reply.started": "2022-02-14T12:11:36.70465Z"
    }
   },
   "outputs": [],
   "source": [
    "data['joblocation_address'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:36.724085Z",
     "iopub.status.busy": "2022-02-14T12:11:36.723913Z",
     "iopub.status.idle": "2022-02-14T12:11:36.731637Z",
     "shell.execute_reply": "2022-02-14T12:11:36.730981Z",
     "shell.execute_reply.started": "2022-02-14T12:11:36.724062Z"
    }
   },
   "outputs": [],
   "source": [
    "def drop_feature(column):\n",
    "    data.drop(column, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:36.733165Z",
     "iopub.status.busy": "2022-02-14T12:11:36.733002Z",
     "iopub.status.idle": "2022-02-14T12:11:36.75096Z",
     "shell.execute_reply": "2022-02-14T12:11:36.750037Z",
     "shell.execute_reply.started": "2022-02-14T12:11:36.733143Z"
    }
   },
   "outputs": [],
   "source": [
    "drop_feature(['payrate', 'experience', 'postdate', 'uniq_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:36.752716Z",
     "iopub.status.busy": "2022-02-14T12:11:36.752349Z",
     "iopub.status.idle": "2022-02-14T12:11:36.759715Z",
     "shell.execute_reply": "2022-02-14T12:11:36.759089Z",
     "shell.execute_reply.started": "2022-02-14T12:11:36.752691Z"
    }
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:11:36.761658Z",
     "iopub.status.busy": "2022-02-14T12:11:36.761278Z",
     "iopub.status.idle": "2022-02-14T12:11:36.789111Z",
     "shell.execute_reply": "2022-02-14T12:11:36.787699Z",
     "shell.execute_reply.started": "2022-02-14T12:11:36.761626Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will convert this data into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T12:13:50.520413Z",
     "iopub.status.busy": "2022-02-14T12:13:50.520201Z",
     "iopub.status.idle": "2022-02-14T12:13:51.446267Z",
     "shell.execute_reply": "2022-02-14T12:13:51.445247Z",
     "shell.execute_reply.started": "2022-02-14T12:13:50.520391Z"
    }
   },
   "outputs": [],
   "source": [
    "data.to_csv('Job Market Analysis.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
